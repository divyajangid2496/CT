package com.availity.spark.provider

import com.github.mrpowers.spark.fast.tests.DataFrameComparer
import org.apache.spark.sql.{Row, SparkSession}
import org.apache.spark.sql.types._
import org.scalatest.BeforeAndAfterAll
import org.scalatest.funsuite.AnyFunSuite

import java.io.{File}
import java.nio.file.Paths

// Shared Test Suite Functionality (as a Trait)
trait SharedTestSuite {
  var spark: SparkSession = _

  def createSparkSession(): SparkSession = {
    SparkSession.builder().master("local").appName("ProviderRosterTest").getOrCreate()
  }

  def beforeAll(): Unit = {
    spark = createSparkSession()
  }

  def afterAll(): Unit = {
    if (spark != null) {
      spark.stop()
    }
  }
}

class ProviderRosterSpec extends AnyFunSuite with DataFrameComparer with BeforeAndAfterAll with SharedTestSuite {

  val baseDir = Paths.get("").toAbsolutePath.toString + "/target/scala-2.12"
  val inputDir = baseDir + "/input_data"
  val outputDir = baseDir + "/output_data"

  // Sample Test Data
  val visitsData = Seq(
    ("visit1", "provider1", "2023-12-01"),
    ("visit2", "provider1", "2023-12-05"),
    ("visit3", "provider2", "2024-01-10")
  )
  val providersData = Seq(
    ("provider1","Bessie","B","Kuphal","Cardiology"),
    ("provider2","Candice","C","Block", "Orthopedics")
  )

  test("Integration Test: Main Pipeline runs without exception") {
    val spark = SparkSession.builder().master("local").appName("ProviderRosterTest").getOrCreate()
    // Define the schema explicitly
    val providerSchema = StructType(Array(
      StructField("provider_id", StringType, true),
      StructField("first_name", StringType, true),
      StructField("last_name", StringType, true),
      StructField("middle_name", StringType, true),
      StructField("provider_specialty", StringType, true)
    ))

    // Define the schema explicitly
    val visitsSchema = StructType(Array(
      StructField("visit_id", StringType, true),
      StructField("provider_id", StringType, true),
      StructField("date_of_service", StringType, true)
    ))

    // Create the visits DataFrame with the explicit schema
    val visitsDF = spark.createDataFrame(
      spark.sparkContext.parallelize(visitsData.map(Row.fromTuple)),
      visitsSchema
    )

    // Create the providers DataFrame with the explicit schema
    val providersDF = spark.createDataFrame(
      spark.sparkContext.parallelize(providersData.map(Row.fromTuple)),
      providerSchema
    )

    // Create input_data/visits directory if it doesn't exist
    val visitsDir = new File(inputDir + "/visits")
    visitsDir.mkdirs()

    // Create input_data/providers directory if it doesn't exist
    val providersDir = new File(inputDir + "/providers")
    providersDir.mkdirs()

    // Write temp input data to the input_data path
    visitsDF.coalesce(1).write.mode("overwrite").option("header", "true").csv(visitsDir.getAbsolutePath)
    providersDF.coalesce(1).write.mode("overwrite").option("header", "true").option("delimiter", "|").csv(providersDir.getAbsolutePath)

    var exceptionThrown = false
    try {
      // Execute the Main Function
      ProviderRoster.main(Array.empty[String])
    } catch {
      case _: Exception => exceptionThrown = true
    }

    assert(!exceptionThrown)  // Assert that no exception was thrown
  }

  test("Test visits per provider output generated by main pipeline") {

    val spark = SparkSession.builder().master("local").appName("ProviderRosterTest").getOrCreate()

    // Define the schema explicitly
    val expectedSchema = StructType(Array(
      StructField("first_name", StringType, true),
      StructField("last_name", StringType, true),
      StructField("middle_name", StringType, true),
      StructField("provider_id", StringType, true),
      StructField("total_visits", LongType, true),
      StructField("provider_specialty", StringType, true)
    ))

    // Expected Results
    val expectedVisitsPerProvider = Seq(
      ("Bessie","B", "Kuphal", "provider1", 2L, "Cardiology"),
      ("Candice","C", "Block", "provider2", 1L, "Orthopedics")
    )

    // Create the expected DataFrame with the explicit schema
    val expectedVisitsPerProviderDF = spark.createDataFrame(
      spark.sparkContext.parallelize(expectedVisitsPerProvider.map(Row.fromTuple)),
      expectedSchema
    )

    // Read Output Data
    val actualVisitsPerProviderDF = spark.read.json(outputDir + "/visits_per_provider/")

    // Assertions
    assertSmallDataFrameEquality(actualVisitsPerProviderDF, expectedVisitsPerProviderDF)
  }

  test ("Test visits per provider per month output generated by main pipeline") {
    val spark = SparkSession.builder().master("local").appName("ProviderRosterTest").getOrCreate()

    // Expected Results for expectedVisitsPerProviderPerMonth
    val expectedVisitsPerProviderPerMonth = Seq(
      (12L, "provider1", 2L),
      (1L, "provider2", 1L)
    )
    // Define the schema explicitly
    val expectedSchemaPerMonth = StructType(Array(
      StructField("month", LongType, true),
      StructField("provider_id", StringType, true),
      StructField("total_visits", LongType, true)
    ))

    // Create the expected DataFrame with the explicit schema
    val expectedVisitsPerMonthDF = spark.createDataFrame(
      spark.sparkContext.parallelize(expectedVisitsPerProviderPerMonth.map(Row.fromTuple)),
      expectedSchemaPerMonth
    )

    // Read Output Data
    val actualVisitsPerMonthDF = spark.read.json(outputDir + "/visits_per_month/")

    // Assertions
    assertSmallDataFrameEquality(actualVisitsPerMonthDF, expectedVisitsPerMonthDF)
  }

  override def beforeAll(): Unit = {
    super.beforeAll()
  }

  override def afterAll(): Unit = {
  }
}
